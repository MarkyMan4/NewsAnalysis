{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sound-sheriff",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "Topic modeling is necessary so that news articles can be grouped by topic. This will provide additional ways to filter through the articles within the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "naval-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel # for evaluating LDA model\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "underlying-terrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/markus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only need to run this once\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "third-beast",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../secrets.json') as file:\n",
    "    secrets = json.load(file)\n",
    "    connection_string = secrets['connection_string']\n",
    "db = create_engine(connection_string)\n",
    "df = pd.read_sql('select * from news_article', con=db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-conference",
   "metadata": {},
   "source": [
    "### code for exporting data from AWS database\n",
    "This was only needed once to move the data to the new database so this can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "distinguished-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date_published'].ffill(inplace=True)\n",
    "# df = df.drop(columns='id')\n",
    "# df = df[df['content'].str.len() > 0]\n",
    "# df = df[df['headline'].str.len() > 0]\n",
    "# df.to_csv('articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-pottery",
   "metadata": {},
   "source": [
    "### Get the content of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "practical-figure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# article1 = df.iloc[-1]['content']\n",
    "# article2 = df.iloc[-2]['content']\n",
    "# articles = [article1, article2]\n",
    "articles = [df.iloc[i]['content'] for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-contents",
   "metadata": {},
   "source": [
    "### Get words from the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "involved-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = word_tokenize(article)\n",
    "# text = nltk.Text(tokens)\n",
    "# text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-emperor",
   "metadata": {},
   "source": [
    "### Show the collacations\n",
    "These are words that appeared consecutively in the text. More specifically, words that appear consecutively and not by chance, so they have meaning when put together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mounted-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-begin",
   "metadata": {},
   "source": [
    "### Tokenize, lemmatize, remove stopwords, stem and discard words fewer than 3 chars\n",
    "- Tokenization involves splitting the article into words.\n",
    "- Lemmatization is getting words into a standard form. Words in third person are changed to first person and verbs are converted to present tense.\n",
    "    - ex: disapperances -> disappearance\n",
    "- Stemming is reducing words to their root form. This also converts all words to lower case.\n",
    "    - ex: disappearance -> disappear\n",
    "- Stopwords are words like \"the\", \"a\", \"an\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "radio-exclusive",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attached-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(article):\n",
    "    tokens = word_tokenize(article.lower()) # make all articles lower case\n",
    "    words = [] # words resulting from applying the filters\n",
    "\n",
    "    for token in tokens:\n",
    "        if len(token) > 3 and token not in stop_words:\n",
    "            words.append(lemmatizer.lemmatize(token))\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "desirable-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_articles = [preprocess(article) for article in articles]\n",
    "# preprocessed_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-external",
   "metadata": {},
   "source": [
    "### Create a dictionary of text and bag of words\n",
    "A bag of words is a list of tuples of the form (token id, count of token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "representative-volunteer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary(preprocessed_articles)\n",
    "corpus = [dictionary.doc2bow(article) for article in preprocessed_articles]\n",
    "\n",
    "# for each tuple in the corpus, the first element is the word index and the second element\n",
    "# is the number of times it appears in the text\n",
    "# for c in corpus:\n",
    "#     for item in c:\n",
    "#         print(f'{item} -- {dictionary[item[0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-worcester",
   "metadata": {},
   "source": [
    "### Create the LDA model for topic modeling\n",
    "This trains a model and creates however many topics are specified. It doesn't assign names to the topics, so these need to be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "relevant-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaMulticore(corpus, num_topics=4, id2word=dictionary, passes=10, workers=2, chunksize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-scene",
   "metadata": {},
   "source": [
    "### Structure the topics as a dict and parse out the words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reverse-badge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"said\" + 0.006*\"china\" + 0.006*\"government\" + 0.006*\"would\" + 0.005*\"state\" + 0.005*\"trump\" + 0.005*\"also\" + 0.005*\"country\" + 0.005*\"president\" + 0.004*\"year\"'),\n",
       " (1,\n",
       "  '0.008*\"said\" + 0.008*\"year\" + 0.006*\"climate\" + 0.004*\"world\" + 0.004*\"change\" + 0.004*\"also\" + 0.003*\"water\" + 0.003*\"animal\" + 0.003*\"area\" + 0.003*\"found\"'),\n",
       " (2,\n",
       "  '0.013*\"said\" + 0.011*\"police\" + 0.007*\"people\" + 0.006*\"protest\" + 0.005*\"navalny\" + 0.005*\"woman\" + 0.004*\"2020\" + 0.004*\"protester\" + 0.004*\"right\" + 0.004*\"officer\"'),\n",
       " (3,\n",
       "  '0.016*\"said\" + 0.010*\"people\" + 0.009*\"covid-19\" + 0.009*\"health\" + 0.008*\"vaccine\" + 0.008*\"coronavirus\" + 0.007*\"case\" + 0.006*\"country\" + 0.006*\"pandemic\" + 0.005*\"virus\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "heavy-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_topics = model.print_topics()\n",
    "topics = {}\n",
    "\n",
    "for topic in raw_topics:\n",
    "    topic_no = topic[0]\n",
    "    topic_words = topic[1].split('+')\n",
    "    \n",
    "    # find words using regex and remove double quotes\n",
    "    topic_words = [re.search('\\\".+\\\"', words).group().replace('\"', '') for words in topic_words]\n",
    "    \n",
    "    # add topic_no and words to topics\n",
    "    topics.update({topic_no: topic_words})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-farming",
   "metadata": {},
   "source": [
    "### Display the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hungry-antarctica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "['said', 'china', 'government', 'would', 'state', 'trump', 'also', 'country', 'president', 'year']\n",
      "\n",
      "topic 1\n",
      "['said', 'year', 'climate', 'world', 'change', 'also', 'water', 'animal', 'area', 'found']\n",
      "\n",
      "topic 2\n",
      "['said', 'police', 'people', 'protest', 'navalny', 'woman', '2020', 'protester', 'right', 'officer']\n",
      "\n",
      "topic 3\n",
      "['said', 'people', 'covid-19', 'health', 'vaccine', 'coronavirus', 'case', 'country', 'pandemic', 'virus']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in topics.keys():\n",
    "    print(f'topic {key}')\n",
    "    print(topics[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-apparel",
   "metadata": {},
   "source": [
    "### Topics\n",
    "0. government/politics\n",
    "1. science\n",
    "2. social\n",
    "3. coronavirus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-neighborhood",
   "metadata": {},
   "source": [
    "### Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "binary-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/news_lda_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-update",
   "metadata": {},
   "source": [
    "### Making predictions with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "agreed-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article = df.iloc[84]['content'] # nature\n",
    "# test_article = df.iloc[3238]['content'] # tech\n",
    "# test_article = df.iloc[5592]['content'] # science\n",
    "# test_article = df.iloc[1339]['content'] # science\n",
    "# test_article = df.iloc[4992]['content'] # business\n",
    "# test_article = df.iloc[27]['content'] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "associate-generic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed = preprocess(test_article)\n",
    "# preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-jamaica",
   "metadata": {},
   "source": [
    "### turn the test article into a bag of words using the existing dicitonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "valued-canberra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.id2word is the dictionary that was used to train the model\n",
    "# this is useful so that I don't have to pickle the dictionary separately\n",
    "bow = model.id2word.doc2bow(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "union-shade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.27824578), (1, 0.010174701), (2, 0.087160416), (3, 0.6244191)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model[bow]\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-honduras",
   "metadata": {},
   "source": [
    "### Find the topic with the best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "prescription-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_topic = pred[0][0]\n",
    "best_match = pred[0][1]\n",
    "\n",
    "for p in pred:\n",
    "    if p[1] > best_match:\n",
    "        predicted_topic = p[0]\n",
    "        best_match = p[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-sword",
   "metadata": {},
   "source": [
    "### The predicted topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sustainable-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "previous-garbage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[df['content'].str.contains('canada')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-dispute",
   "metadata": {},
   "source": [
    "### some helpful methods to use with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "impressive-mother",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.27820295), (1, 0.010230508), (2, 0.08716508), (3, 0.6244015)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show topics for a specific document\n",
    "model.get_document_topics(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "expanded-calgary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.29945651e-05, 1.31194794e-03, 2.41260670e-04, ...,\n",
       "        9.85758675e-07, 3.64825723e-07, 3.64825723e-07],\n",
       "       [1.13724534e-04, 1.08553912e-03, 9.24471649e-04, ...,\n",
       "        8.75822025e-07, 8.76245963e-07, 8.76245963e-07],\n",
       "       [8.34769526e-05, 5.16337877e-07, 6.42332743e-05, ...,\n",
       "        1.67864425e-06, 3.42393264e-06, 3.42393264e-06],\n",
       "       [3.48626927e-04, 1.81667809e-03, 1.08105596e-03, ...,\n",
       "        5.58041961e-07, 6.70242457e-07, 6.70242457e-07]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get probability of each word in each topic\n",
    "model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "subsequent-expression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 0.015941896),\n",
       " ('china', 0.006209723),\n",
       " ('government', 0.0061553097),\n",
       " ('would', 0.005607035),\n",
       " ('state', 0.0051090037),\n",
       " ('trump', 0.0049358923),\n",
       " ('also', 0.0047460306),\n",
       " ('country', 0.004741364),\n",
       " ('president', 0.00473833),\n",
       " ('year', 0.004197876),\n",
       " ('u.s.', 0.0041286964),\n",
       " ('company', 0.0039096377),\n",
       " ('report', 0.003684636),\n",
       " ('biden', 0.0035926218),\n",
       " ('chinese', 0.0033445929),\n",
       " ('official', 0.0031176542),\n",
       " ('right', 0.0030737636),\n",
       " ('time', 0.002836811),\n",
       " ('people', 0.0027904343),\n",
       " ('united', 0.002661332)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top words for a specific topic.\n",
    "# I believe the number along with each word is the probability that a document would be part\n",
    "# of that topic if it contained that word, but I would need to double check that in the docs.\n",
    "model.show_topic(0, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "falling-corpus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.015775748, 'said'),\n",
       "   (0.009961982, 'people'),\n",
       "   (0.008805061, 'covid-19'),\n",
       "   (0.008625574, 'health'),\n",
       "   (0.007893125, 'vaccine'),\n",
       "   (0.00754997, 'coronavirus'),\n",
       "   (0.0065441034, 'case'),\n",
       "   (0.0061498303, 'country'),\n",
       "   (0.005538134, 'pandemic'),\n",
       "   (0.0054097585, 'virus'),\n",
       "   (0.004563001, 'government'),\n",
       "   (0.0040496625, 'first'),\n",
       "   (0.0039926535, 'death'),\n",
       "   (0.003987456, 'also'),\n",
       "   (0.0038391184, 'would'),\n",
       "   (0.0038302927, 'week'),\n",
       "   (0.0034949782, 'number'),\n",
       "   (0.0033850733, 'time'),\n",
       "   (0.00330971, 'home'),\n",
       "   (0.0032336428, 'public')],\n",
       "  -0.8262141936458304),\n",
       " ([(0.015941896, 'said'),\n",
       "   (0.006209723, 'china'),\n",
       "   (0.0061553097, 'government'),\n",
       "   (0.005607035, 'would'),\n",
       "   (0.0051090037, 'state'),\n",
       "   (0.0049358923, 'trump'),\n",
       "   (0.0047460306, 'also'),\n",
       "   (0.004741364, 'country'),\n",
       "   (0.00473833, 'president'),\n",
       "   (0.004197876, 'year'),\n",
       "   (0.0041286964, 'u.s.'),\n",
       "   (0.0039096377, 'company'),\n",
       "   (0.003684636, 'report'),\n",
       "   (0.0035926218, 'biden'),\n",
       "   (0.0033445929, 'chinese'),\n",
       "   (0.0031176542, 'official'),\n",
       "   (0.0030737636, 'right'),\n",
       "   (0.002836811, 'time'),\n",
       "   (0.0027904343, 'people'),\n",
       "   (0.002661332, 'united')],\n",
       "  -1.0558735858457473),\n",
       " ([(0.0077930638, 'said'),\n",
       "   (0.0075588664, 'year'),\n",
       "   (0.0063611856, 'climate'),\n",
       "   (0.004451543, 'world'),\n",
       "   (0.0038467292, 'change'),\n",
       "   (0.0036619036, 'also'),\n",
       "   (0.0033292756, 'water'),\n",
       "   (0.0033052114, 'animal'),\n",
       "   (0.0032911361, 'area'),\n",
       "   (0.0032673893, 'found'),\n",
       "   (0.0031419103, 'scientist'),\n",
       "   (0.002978622, 'time'),\n",
       "   (0.002823921, 'study'),\n",
       "   (0.0028172396, 'human'),\n",
       "   (0.0027996586, 'global'),\n",
       "   (0.0027679869, 'could'),\n",
       "   (0.0027167052, 'would'),\n",
       "   (0.0026975134, 'research'),\n",
       "   (0.0024639743, 'specie'),\n",
       "   (0.0024630935, 'researcher')],\n",
       "  -1.356400797092763),\n",
       " ([(0.013369357, 'said'),\n",
       "   (0.010996579, 'police'),\n",
       "   (0.00729306, 'people'),\n",
       "   (0.0059129503, 'protest'),\n",
       "   (0.004896369, 'navalny'),\n",
       "   (0.0047229645, 'woman'),\n",
       "   (0.0043838765, '2020'),\n",
       "   (0.0038131, 'protester'),\n",
       "   (0.0035773905, 'right'),\n",
       "   (0.0035036458, 'officer'),\n",
       "   (0.0034616296, 'news'),\n",
       "   (0.0032820844, 'year'),\n",
       "   (0.0032702438, 'also'),\n",
       "   (0.0032276313, 'city'),\n",
       "   (0.0029375695, 'force'),\n",
       "   (0.0029218295, 'group'),\n",
       "   (0.0028107627, 'video'),\n",
       "   (0.0027935526, 'court'),\n",
       "   (0.002669654, 'told'),\n",
       "   (0.0025413018, 'myanmar')],\n",
       "  -1.4136499236402498)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show each topic and it's coherence score. This will be useful for evaluating the model.\n",
    "model.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-dinner",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "julian-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create several LDA models with different k values\n",
    "model1 = LdaMulticore(corpus, num_topics=3, id2word=dictionary, passes=5, workers=2, chunksize=100)\n",
    "model2 = LdaMulticore(corpus, num_topics=4, id2word=dictionary, passes=5, workers=2, chunksize=100)\n",
    "model3 = LdaMulticore(corpus, num_topics=5, id2word=dictionary, passes=5, workers=2, chunksize=100)\n",
    "model4 = LdaMulticore(corpus, num_topics=6, id2word=dictionary, passes=5, workers=2, chunksize=100)\n",
    "model5 = LdaMulticore(corpus, num_topics=7, id2word=dictionary, passes=5, workers=2, chunksize=100)\n",
    "model6 = LdaMulticore(corpus, num_topics=8, id2word=dictionary, passes=5, workers=2, chunksize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "entire-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create coherence models to evaluate each LDA model\n",
    "cm1 = CoherenceModel(model=model1, corpus=corpus, coherence='u_mass')\n",
    "cm2 = CoherenceModel(model=model2, corpus=corpus, coherence='u_mass')\n",
    "cm3 = CoherenceModel(model=model3, corpus=corpus, coherence='u_mass')\n",
    "cm4 = CoherenceModel(model=model4, corpus=corpus, coherence='u_mass')\n",
    "cm5 = CoherenceModel(model=model5, corpus=corpus, coherence='u_mass')\n",
    "cm6 = CoherenceModel(model=model6, corpus=corpus, coherence='u_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "black-exhibition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 topics: -1.230026945660962\n",
      "4 topics: -1.1466544784621742\n",
      "5 topics: -1.3507580140997066\n",
      "6 topics: -1.4155287392165115\n",
      "7 topics: -2.9326326634684228\n",
      "8 topics: -1.47439138870618\n"
     ]
    }
   ],
   "source": [
    "# show the coherence score for each model, the score closest to 0 is the best model\n",
    "print(f'3 topics: {cm1.get_coherence()}')\n",
    "print(f'4 topics: {cm2.get_coherence()}')\n",
    "print(f'5 topics: {cm3.get_coherence()}')\n",
    "print(f'6 topics: {cm4.get_coherence()}')\n",
    "print(f'7 topics: {cm5.get_coherence()}')\n",
    "print(f'8 topics: {cm6.get_coherence()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-music",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
